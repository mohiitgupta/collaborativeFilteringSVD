{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and split data, create our own MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv(\n",
    "    './ml-100k/u.data', sep='\\t', names=names)\n",
    "\n",
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "\n",
    "# Create r_{ui}, our ratings matrix\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "\n",
    "# Split into training and test sets. \n",
    "# Remove 10 ratings for each user \n",
    "# and assign them to the test set\n",
    "def train_test_split(ratings):\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                        size=10, \n",
    "                                        replace=False)\n",
    "        train[user, test_ratings] = 0.\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test\n",
    "\n",
    "train, test = train_test_split(ratings)\n",
    "indicating_mat = np.vectorize(lambda x: 0 if x==0 else 1)(train)\n",
    "mask = indicating_mat == 1\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user, n_item = train.shape\n",
    "\n",
    "# Hyperparameter:\n",
    "k = 40\n",
    "steps = 400\n",
    "lrate = 0.001\n",
    "_lambda = 0.001\n",
    "\n",
    "# Parameters:\n",
    "b_u = np.zeros(n_user)\n",
    "b_i = np.zeros(n_item)\n",
    "\n",
    "#q_i\n",
    "q_i = np.random.normal(scale=1./k, size=(n_item, k))\n",
    "#P_u\n",
    "x_j = np.random.normal(scale=1./k, size=(n_item, k))\n",
    "#y_i\n",
    "y_j = np.random.normal(scale=1./k, size=(n_item, k))\n",
    "\n",
    "global_bias = np.mean(train[np.where(train != 0)])\n",
    "\n",
    "non_zeros = train.nonzero()\n",
    "\n",
    "# This is equivalent of taking the length and doing the square root.\n",
    "# N = np.power(indicating_mat.sum(1), -0.5)\n",
    "N = 1./np.linalg.norm(indicating_mat, axis = 1)\n",
    "R = 1./np.linalg.norm(train, axis = 1)\n",
    "\n",
    "\n",
    "def get_item_item (u, mu, b_i, b_u, x_j):\n",
    "    temp = np.zeros(k)\n",
    "    for j in indicating_mat[u,:].nonzero()[0]:\n",
    "        temp += (ratings[u, j] - mu - b_u[u] - b_i[j])*x_j[j, :]\n",
    "    return temp * R[u]\n",
    "        \n",
    "def predict_one(u, i, b_i, b_u, q_i, x_j, y_j):\n",
    "    item_item = get_item_item(u, global_bias, b_i, b_u, x_j)\n",
    "    return global_bias + b_i[i] + b_u[u] + \\\n",
    "    q_i[i,:].T.dot(item_item + N[u] * y_j[mask[u,:], :].sum(axis=0))\n",
    "\n",
    "def svdasy_step():\n",
    "    rows = np.random.permutation(len(non_zeros[0]))\n",
    "    for i in rows:\n",
    "        user = non_zeros[0][i]\n",
    "        item = non_zeros[1][i]\n",
    "        pred = predict_one(user, item, b_i, b_u, q_i, x_j, y_j)\n",
    "\n",
    "        ## Watch out to turn learning rate separately, this needs to be calculate separately\n",
    "        error = train[user][item] - pred\n",
    "        \n",
    "        b_u[user] += lrate*(error - _lambda * b_u[user])\n",
    "        b_i[item] += lrate*(error - _lambda * b_i[item])\n",
    "        \n",
    "        \n",
    "        item_item = get_item_item(user, global_bias, b_i, b_u, x_j)\n",
    "        ## Update for q_i (item vector)\n",
    "        q_i[item, :] += lrate * (error * (item_item + N[user]* y_j[mask[user, :], :].sum(axis=0)) -\\\n",
    "                                  _lambda * q_i[item, :])\n",
    "        \n",
    "        # Update for x_j (user vector)\n",
    "        j = indicating_mat[user,:].nonzero()\n",
    "        x_j[j, :] += lrate * (error * item_item * q_i[item, :] - _lambda * x_j[j, :])\n",
    "        \n",
    "        # Update for each y_j \n",
    "        temp = error * N[user] * q_i[item, :]\n",
    "        j = indicating_mat[user,:].nonzero()\n",
    "        y_j[j,:] += lrate * (temp - _lambda * y_j[j,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ratings[1, 1]- global_bias - b_u[0] - b_i[0]) * x_j[0, :] + np.zeros(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "def predict():\n",
    "    predictions = np.zeros([n_user, n_item])\n",
    "    for user in range(n_users):\n",
    "        for item in range(n_items):\n",
    "            predictions[user, item] = predict_one(user, item, b_i, \\\n",
    "                        b_u, q_i, x_j, y_j)\n",
    "    data.append([get_mse(predictions, train),get_mse(predictions, test)])\n",
    "    with open('./asym.csv', 'a') as f:\n",
    "        f.write(get_mse(predictions, train)+','+get_mse(predictions, test)+' \\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(steps):\n",
    "    print(i)\n",
    "    svdasy_step()\n",
    "    if i%10 == 0:\n",
    "        predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtf = pd.DataFrame(data)\n",
    "dtf.to_csv(\"asymResult.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_mse(predictions, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_mse(predictions, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('./asymResult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
